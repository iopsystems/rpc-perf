# An example configuration for benchmarking Momento (https://www.gomomento.com)
# and demonstrating the use of the preview functionality for collections. Each
# command family is using its own keyspace and covers key-value, hash, list,
# set, and sorted set.
#
# Expiration: unless otherwise specified, the default TTL of 15 minutes will be
# used. Commands which operate on collections will not refresh the TTL for the
# collection.

[general]
# specify the protocol to be used
protocol = "s3"
# the interval for stats integration and reporting
interval = 1
# the number of intervals to run the test for
duration = 30
# run the admin thread with a HTTP listener at the address provided, this allows
# stats exposition via HTTP
admin = "127.0.0.1:9090"
# optionally, set an initial seed for the PRNGs used to generate the workload.
# The default is to intialize from the OS entropy pool.
initial_seed = "0"

#[metrics]
# output file for detailed stats during the run
#output = "stats.json"
# format of the output file (possible values are json, msgpack, parquet)
#format = "json"
# optionally specify batch size for parquet row groups
# only valid for parquet output
#batch_size = 100_000
# optionally specify histogram type (can be standard (default) or sparse)
# only valid for parquet output
#histogram = "sparse"
# optionally, specify the sampling interval for metrics. Input is a string
# with the unit attached; for example "100ms" or "1s". Defaults to 1s.
#interval = "1s"

[debug]
# choose from: error, warn, info, debug, trace
log_level = "info"
# optionally, log to the file below instead of standard out
# log_file = "rpc-perf.log"
# backup file name for use with log rotation
log_backup = "rpc-perf.log.old"
# trigger log rotation when the file grows beyond this size (in bytes). Set this
# option to '0' to disable log rotation.
log_max_size = 1073741824

[target]
# we provide a vHost bucket URL as the target:
endpoints = ["https://[BUCKET_NAME].s3.[REGION].amazonaws.com"]

[storage]
# number of threads used to drive requests
threads = 4
# number of concurrent connections to have open to S3. Since the underlying
# protocol is HTTP/1.1, there is no mux/concurrency per connection.
poolsize = 1

# currently these timeouts are ignored but are required config fields
request_timeout = 1000
connect_timeout = 100000

[workload]
# the number of threads that will be used to generate the workload
threads = 1

[workload.ratelimit]
# set a global ratelimit for the workload
start = 1

[[workload.stores]]
# object name length, in bytes
klen = 32
# sets the number of objects that will be generated
nkeys = 100
# sets the object size, in bytes
vlen = 128
# controls what commands will be used in this keyspace
commands = [
	# get a value
	{ verb = "get", weight = 80 },
	# set a value
	{ verb = "put", weight = 19 },
	# # delete a value
	{ verb = "delete", weight = 1 },
]
